train_objective: "mnr" # "mnr", "pair-class", "triplet"
run_name: "mnr_attentiv_last_8h_8q" # "triplet_max_pooling_bert"

datasets:
  mnli:
    name: "LysandreJik/glue-mnli-train"
  all-nli:
    name: "sentence-transformers/all-nli"
    subset: "pair-class" # "triplet"

training:
  batch_size: 32
  learning_rate: 2e-5
  epochs: 5
  pooling_learning_rate: 2e-5 # Used only in attention pooling


embedder:
  pooling: "attention_last_layer" # "mean", "max" , "attention_last_layer", "attention_multi_layer"


# Usedif there is attention pooling
heads: 8
queries_per_head: 8
multi_layer_pooling:
  last_n_layers: 10

models_root: "/storage/brno2/home/jirin/NLP/embbedings_project/output" # change this to your model folder path
output_folder: "test_results" # "runs"
tasks:
  # Semantic similarity (English Only)
  - "STS12"
  - "STS13" 
  - "STS14"
  - "STS15"
  - "STS16"
  - "STSBenchmark" # This is the main one
  
  # Retrieval
  - "NQ"
  - "MSMARCO"
  - "QuoraRetrieval"
    
  # Classification
  - "Banking77Classification"
  - "StackExchangeClustering"