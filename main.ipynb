{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5aebb9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Bahno\\.conda\\envs\\zpja_cop_project\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['anchor', 'positive', 'negative'],\n",
      "        num_rows: 557850\n",
      "    })\n",
      "    dev: Dataset({\n",
      "        features: ['anchor', 'positive', 'negative'],\n",
      "        num_rows: 6584\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['anchor', 'positive', 'negative'],\n",
      "        num_rows: 6609\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# TODO test if validation is written correctly\n",
    "# TODO testing on benchmark\n",
    "# TODO tracking and charting val and train loss over training\n",
    "# TODO sicne we test on benchmark, we can add testing data to val or train\n",
    "# TODO also check setings 'pair-class', 'pair-score','triplet', 'pair'\n",
    "dataset = load_dataset(\"sentence-transformers/all-nli\",'triplet')\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c58aa18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "class BertSentenceEmbedder(nn.Module):\n",
    "    def __init__(self, model_name=\"bert-base-uncased\"):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        token_embeddings = outputs.last_hidden_state  # (batch, seq_len, hidden)\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size())\n",
    "\n",
    "\n",
    "        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "        token_counts = input_mask_expanded.sum(1)\n",
    "        # avoid division by zero, if empty sentence\n",
    "        token_counts = torch.clamp(token_counts, min=1e-9)\n",
    "        return sum_embeddings / token_counts # compute mean\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7b0c92e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c28c022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Bahno\\.conda\\envs\\zpja_cop_project\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 0.0000 | Val Loss: 0.7395\n",
      "Epoch 1: Loss = 0.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 93\u001b[0m\n\u001b[0;32m     91\u001b[0m avg_epoch_loss \u001b[38;5;241m=\u001b[39m total_epoch_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader)\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Loss = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_epoch_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 93\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m validate(model,val_loader)\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_epoch_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Val Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     95\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n",
      "Cell \u001b[1;32mIn[5], line 32\u001b[0m, in \u001b[0;36mvalidate\u001b[1;34m(model, val_loader)\u001b[0m\n\u001b[0;32m     30\u001b[0m total_val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 32\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m enc_anchor, enc_pos, enc_neg \u001b[38;5;129;01min\u001b[39;00m val_loader:\n\u001b[0;32m     33\u001b[0m         input_ids_a, attn_a \u001b[38;5;241m=\u001b[39m enc_anchor[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m], enc_anchor[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     34\u001b[0m         input_ids_p, attn_p \u001b[38;5;241m=\u001b[39m enc_pos[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m], enc_pos[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Bahno\\.conda\\envs\\zpja_cop_project\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:626\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m--> 626\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_profile_name):\n\u001b[0;32m    627\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m             \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Bahno\\.conda\\envs\\zpja_cop_project\\Lib\\site-packages\\torch\\autograd\\profiler.py:648\u001b[0m, in \u001b[0;36mrecord_function.__exit__\u001b[1;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[0;32m    646\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting():\n\u001b[0;32m    647\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mDisableTorchFunctionSubclass():\n\u001b[1;32m--> 648\u001b[0m         torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39m_record_function_exit\u001b[38;5;241m.\u001b[39m_RecordFunction(record)\n\u001b[0;32m    649\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    650\u001b[0m     torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39m_record_function_exit(record)\n",
      "File \u001b[1;32mc:\\Users\\Bahno\\.conda\\envs\\zpja_cop_project\\Lib\\site-packages\\torch\\_ops.py:448\u001b[0m, in \u001b[0;36mOpOverload.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 448\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_op(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs \u001b[38;5;129;01mor\u001b[39;00m {})\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer\n",
    "from torch.nn import TripletMarginLoss\n",
    "loss_fn = TripletMarginLoss(margin=1.0, p=2)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch_size = len(batch)\n",
    "    anchors = [b[\"anchor\"] for b in batch]\n",
    "    positives = [b[\"positive\"] for b in batch]\n",
    "    negatives = [b[\"negative\"] for b in batch]\n",
    "\n",
    "    # Combine into one list\n",
    "    all_sentences = anchors + positives + negatives\n",
    "    # Tokenize in one pass\n",
    "    enc_all = tokenizer(all_sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    # Then split back again\n",
    "    enc_anchor = {k: v[:batch_size] for k, v in enc_all.items()}\n",
    "    enc_pos = {k: v[batch_size:2*batch_size] for k, v in enc_all.items()}\n",
    "    enc_neg = {k: v[2*batch_size:] for k, v in enc_all.items()}\n",
    "\n",
    "\n",
    "    # each is dict of of 'input_ids', 'token_type_ids', 'attention_mask' which are tensors\n",
    "    return enc_anchor, enc_pos, enc_neg\n",
    "\n",
    "\n",
    "def validate(model,val_loader):\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for enc_anchor, enc_pos, enc_neg in val_loader:\n",
    "            input_ids_a, attn_a = enc_anchor[\"input_ids\"], enc_anchor[\"attention_mask\"]\n",
    "            input_ids_p, attn_p = enc_pos[\"input_ids\"], enc_pos[\"attention_mask\"]\n",
    "            input_ids_n, attn_n = enc_neg[\"input_ids\"], enc_neg[\"attention_mask\"]\n",
    "\n",
    "            all_input_ids = torch.cat([input_ids_a, input_ids_p, input_ids_n], dim=0).to(device)\n",
    "            all_attention_mask = torch.cat([attn_a, attn_p, attn_n], dim=0).to(device)\n",
    "\n",
    "            all_embeddings = model(all_input_ids, all_attention_mask)\n",
    "            emb_a = all_embeddings[:batch_size]\n",
    "            emb_p = all_embeddings[batch_size:2*batch_size]\n",
    "            emb_n = all_embeddings[2*batch_size:]\n",
    "\n",
    "            loss = loss_fn(emb_a, emb_p, emb_n)\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    return avg_val_loss\n",
    "\n",
    "batch_size=16\n",
    "\n",
    "train_loader = DataLoader(dataset[\"train\"], batch_size=batch_size, shuffle=True, collate_fn=collate_fn,drop_last=True)\n",
    "val_loader = DataLoader(dataset[\"dev\"], batch_size=batch_size, shuffle=False, collate_fn=collate_fn,drop_last=True)\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "model = BertSentenceEmbedder().to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    total_epoch_loss = 0\n",
    "    for enc_anchor, enc_pos, enc_neg in train_loader:\n",
    "        # Extract input_ids and attention_mask for each\n",
    "        input_ids_a, attn_a = enc_anchor[\"input_ids\"], enc_anchor[\"attention_mask\"]\n",
    "        input_ids_p, attn_p = enc_pos[\"input_ids\"], enc_pos[\"attention_mask\"]\n",
    "        input_ids_n, attn_n = enc_neg[\"input_ids\"], enc_neg[\"attention_mask\"]\n",
    "\n",
    "        # Concatenate input_ids and attention_mask for single forward pass\n",
    "        all_input_ids = torch.cat([input_ids_a, input_ids_p, input_ids_n], dim=0).to(device)\n",
    "        all_attention_mask = torch.cat([attn_a, attn_p, attn_n], dim=0).to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        all_embeddings = model(all_input_ids, all_attention_mask)\n",
    "        # Split embeddings back into anchor, positive, negative\n",
    "        emb_a = all_embeddings[:batch_size]\n",
    "        emb_p = all_embeddings[batch_size:2*batch_size]\n",
    "        emb_n = all_embeddings[2*batch_size:]\n",
    "\n",
    "        # Compute triplet loss\n",
    "        loss = loss_fn(emb_a, emb_p, emb_n)\n",
    "\n",
    "        # Backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_epoch_loss += loss.item()\n",
    "    avg_epoch_loss = total_epoch_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch}: Loss = {avg_epoch_loss:.4f}\")\n",
    "    val_loss = validate(model,val_loader)\n",
    "    print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {avg_epoch_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "    model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33d398c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"triplet_loss_bert.pt\"\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8dca4583",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mteb.types import PromptType,BatchedInput\n",
    "from mteb.abstasks.task_metadata import TaskMetadata\n",
    "\n",
    "#TODO test it the tokenizer returns the correct shapes and etc.\n",
    "# MTEB expects model with 'encode' method, that takes in list of sentences, and returns numpy embeddings\n",
    "class BenchmarkEncoder:\n",
    "    def __init__(self, model, tokenizer, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "        self.model = model.to(device)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        self.model.eval()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def encode(\n",
    "        self,\n",
    "        inputs: DataLoader[BatchedInput],\n",
    "        task_metadata: TaskMetadata,\n",
    "        hf_split: str,\n",
    "        hf_subset: str,\n",
    "        prompt_type: PromptType | None = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "   \n",
    "        \"\"\"Encodes the given sentences using the encoder.\n",
    "\n",
    "        Args:\n",
    "            inputs: The inputs to encode.\n",
    "            task_metadata: The name of the task.\n",
    "            hf_subset: The subset of the dataset.\n",
    "            hf_split: The split of the dataset.\n",
    "            prompt_type: The prompt type to use.\n",
    "            **kwargs: Additional arguments to pass to the encoder.\n",
    "\n",
    "        Returns:\n",
    "            The encoded sentences.\n",
    "        \"\"\"\n",
    "        batch_size = kwargs['batch_size']\n",
    "        \n",
    "        all_embeddings = []\n",
    "\n",
    "        for batch in inputs:\n",
    "            enc = self.tokenizer(batch['text'], padding=True, truncation=True, return_tensors=\"pt\").to(self.device)\n",
    "            embeddings = self.model(enc[\"input_ids\"], enc[\"attention_mask\"])\n",
    "            all_embeddings.append(embeddings)\n",
    "\n",
    "\n",
    "        return torch.cat(all_embeddings,dim=0).cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1cc4695b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bahno\\AppData\\Local\\Temp\\ipykernel_17376\\1589277689.py:10: DeprecationWarning: MTEB is deprecated and will be removed in future versions. Please use the `mteb.evaluate` function instead.\n",
      "  evaluation = mteb.MTEB(tasks=tasks)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #262626; text-decoration-color: #262626\">───────────────────────────────────────────────── </span><span style=\"font-weight: bold\">Selected tasks </span><span style=\"color: #262626; text-decoration-color: #262626\"> ─────────────────────────────────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;5;235m───────────────────────────────────────────────── \u001b[0m\u001b[1mSelected tasks \u001b[0m\u001b[38;5;235m ─────────────────────────────────────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">STS</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mSTS\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">    - HUMESICK-R, <span style=\"color: #626262; text-decoration-color: #626262; font-style: italic\">t2t</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "    - HUMESICK-R, \u001b[3;38;5;241mt2t\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">    - HUMESTS12, <span style=\"color: #626262; text-decoration-color: #626262; font-style: italic\">t2t</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "    - HUMESTS12, \u001b[3;38;5;241mt2t\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">    - STS12, <span style=\"color: #626262; text-decoration-color: #626262; font-style: italic\">t2t</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "    - STS12, \u001b[3;38;5;241mt2t\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">    - STS13, <span style=\"color: #626262; text-decoration-color: #626262; font-style: italic\">t2t</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "    - STS13, \u001b[3;38;5;241mt2t\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">    - STS14, <span style=\"color: #626262; text-decoration-color: #626262; font-style: italic\">t2t</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "    - STS14, \u001b[3;38;5;241mt2t\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">    - STS15, <span style=\"color: #626262; text-decoration-color: #626262; font-style: italic\">t2t</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "    - STS15, \u001b[3;38;5;241mt2t\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">    - STS16, <span style=\"color: #626262; text-decoration-color: #626262; font-style: italic\">t2t</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "    - STS16, \u001b[3;38;5;241mt2t\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">    - BIOSSES, <span style=\"color: #626262; text-decoration-color: #626262; font-style: italic\">t2t</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "    - BIOSSES, \u001b[3;38;5;241mt2t\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">    - HUMESTSBenchmark, <span style=\"color: #626262; text-decoration-color: #626262; font-style: italic\">t2t</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "    - HUMESTSBenchmark, \u001b[3;38;5;241mt2t\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">    - STSBenchmark, <span style=\"color: #626262; text-decoration-color: #626262; font-style: italic\">t2t</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "    - STSBenchmark, \u001b[3;38;5;241mt2t\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">    - SICK-R, <span style=\"color: #626262; text-decoration-color: #626262; font-style: italic\">t2t</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "    - SICK-R, \u001b[3;38;5;241mt2t\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">    - HUMESTS22, <span style=\"color: #626262; text-decoration-color: #626262; font-style: italic\">t2t</span>, <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">multilingual </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold; font-style: italic\">1</span><span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\"> / </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold; font-style: italic\">4</span><span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\"> Subsets</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "    - HUMESTS22, \u001b[3;38;5;241mt2t\u001b[0m, \u001b[3;31mmultilingual \u001b[0m\u001b[1;3;31m1\u001b[0m\u001b[3;31m \u001b[0m\u001b[3;31m/\u001b[0m\u001b[3;31m \u001b[0m\u001b[1;3;31m4\u001b[0m\u001b[3;31m Subsets\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">    - IndicCrosslingualSTS, <span style=\"color: #626262; text-decoration-color: #626262; font-style: italic\">t2t</span>, <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">multilingual </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold; font-style: italic\">12</span><span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\"> / </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold; font-style: italic\">12</span><span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\"> Subsets</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "    - IndicCrosslingualSTS, \u001b[3;38;5;241mt2t\u001b[0m, \u001b[3;31mmultilingual \u001b[0m\u001b[1;3;31m12\u001b[0m\u001b[3;31m \u001b[0m\u001b[3;31m/\u001b[0m\u001b[3;31m \u001b[0m\u001b[1;3;31m12\u001b[0m\u001b[3;31m Subsets\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">    - STS17, <span style=\"color: #626262; text-decoration-color: #626262; font-style: italic\">t2t</span>, <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">multilingual </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold; font-style: italic\">8</span><span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\"> / </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold; font-style: italic\">11</span><span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\"> Subsets</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "    - STS17, \u001b[3;38;5;241mt2t\u001b[0m, \u001b[3;31mmultilingual \u001b[0m\u001b[1;3;31m8\u001b[0m\u001b[3;31m \u001b[0m\u001b[3;31m/\u001b[0m\u001b[3;31m \u001b[0m\u001b[1;3;31m11\u001b[0m\u001b[3;31m Subsets\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">    - STS22.v2, <span style=\"color: #626262; text-decoration-color: #626262; font-style: italic\">t2t</span>, <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">multilingual </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold; font-style: italic\">5</span><span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\"> / </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold; font-style: italic\">18</span><span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\"> Subsets</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "    - STS22.v2, \u001b[3;38;5;241mt2t\u001b[0m, \u001b[3;31mmultilingual \u001b[0m\u001b[1;3;31m5\u001b[0m\u001b[3;31m \u001b[0m\u001b[3;31m/\u001b[0m\u001b[3;31m \u001b[0m\u001b[1;3;31m18\u001b[0m\u001b[3;31m Subsets\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">    - STSBenchmarkMultilingualSTS, <span style=\"color: #626262; text-decoration-color: #626262; font-style: italic\">t2t</span>, <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">multilingual </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold; font-style: italic\">1</span><span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\"> / </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold; font-style: italic\">10</span><span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\"> Subsets</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "    - STSBenchmarkMultilingualSTS, \u001b[3;38;5;241mt2t\u001b[0m, \u001b[3;31mmultilingual \u001b[0m\u001b[1;3;31m1\u001b[0m\u001b[3;31m \u001b[0m\u001b[3;31m/\u001b[0m\u001b[3;31m \u001b[0m\u001b[1;3;31m10\u001b[0m\u001b[3;31m Subsets\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">    - SemRel24STS, <span style=\"color: #626262; text-decoration-color: #626262; font-style: italic\">t2t</span>, <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">multilingual </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold; font-style: italic\">1</span><span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\"> / </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold; font-style: italic\">12</span><span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\"> Subsets</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "    - SemRel24STS, \u001b[3;38;5;241mt2t\u001b[0m, \u001b[3;31mmultilingual \u001b[0m\u001b[1;3;31m1\u001b[0m\u001b[3;31m \u001b[0m\u001b[3;31m/\u001b[0m\u001b[3;31m \u001b[0m\u001b[1;3;31m12\u001b[0m\u001b[3;31m Subsets\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "c:\\Users\\Bahno\\.conda\\envs\\zpja_cop_project\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "c:\\Users\\Bahno\\.conda\\envs\\zpja_cop_project\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "c:\\Users\\Bahno\\.conda\\envs\\zpja_cop_project\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "c:\\Users\\Bahno\\.conda\\envs\\zpja_cop_project\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "c:\\Users\\Bahno\\.conda\\envs\\zpja_cop_project\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "c:\\Users\\Bahno\\.conda\\envs\\zpja_cop_project\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "c:\\Users\\Bahno\\.conda\\envs\\zpja_cop_project\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "c:\\Users\\Bahno\\.conda\\envs\\zpja_cop_project\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "c:\\Users\\Bahno\\.conda\\envs\\zpja_cop_project\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "c:\\Users\\Bahno\\.conda\\envs\\zpja_cop_project\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "c:\\Users\\Bahno\\.conda\\envs\\zpja_cop_project\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "c:\\Users\\Bahno\\.conda\\envs\\zpja_cop_project\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "c:\\Users\\Bahno\\.conda\\envs\\zpja_cop_project\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "c:\\Users\\Bahno\\.conda\\envs\\zpja_cop_project\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "c:\\Users\\Bahno\\.conda\\envs\\zpja_cop_project\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "c:\\Users\\Bahno\\.conda\\envs\\zpja_cop_project\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "c:\\Users\\Bahno\\.conda\\envs\\zpja_cop_project\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "c:\\Users\\Bahno\\.conda\\envs\\zpja_cop_project\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "c:\\Users\\Bahno\\.conda\\envs\\zpja_cop_project\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "c:\\Users\\Bahno\\.conda\\envs\\zpja_cop_project\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "c:\\Users\\Bahno\\.conda\\envs\\zpja_cop_project\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "c:\\Users\\Bahno\\.conda\\envs\\zpja_cop_project\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "c:\\Users\\Bahno\\.conda\\envs\\zpja_cop_project\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "c:\\Users\\Bahno\\.conda\\envs\\zpja_cop_project\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "c:\\Users\\Bahno\\.conda\\envs\\zpja_cop_project\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "c:\\Users\\Bahno\\.conda\\envs\\zpja_cop_project\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "c:\\Users\\Bahno\\.conda\\envs\\zpja_cop_project\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "c:\\Users\\Bahno\\.conda\\envs\\zpja_cop_project\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "c:\\Users\\Bahno\\.conda\\envs\\zpja_cop_project\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "c:\\Users\\Bahno\\.conda\\envs\\zpja_cop_project\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "c:\\Users\\Bahno\\.conda\\envs\\zpja_cop_project\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "c:\\Users\\Bahno\\.conda\\envs\\zpja_cop_project\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "c:\\Users\\Bahno\\.conda\\envs\\zpja_cop_project\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "c:\\Users\\Bahno\\.conda\\envs\\zpja_cop_project\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "c:\\Users\\Bahno\\.conda\\envs\\zpja_cop_project\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "c:\\Users\\Bahno\\.conda\\envs\\zpja_cop_project\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "c:\\Users\\Bahno\\.conda\\envs\\zpja_cop_project\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "c:\\Users\\Bahno\\.conda\\envs\\zpja_cop_project\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "c:\\Users\\Bahno\\.conda\\envs\\zpja_cop_project\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "c:\\Users\\Bahno\\.conda\\envs\\zpja_cop_project\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import mteb\n",
    "\n",
    "wrapped = BenchmarkEncoder(model, tokenizer)\n",
    "\n",
    "# task_types=[\"Clustering\", \"Retrieval\"]\n",
    "# tasks = mteb.get_tasks(tasks=[\"STS16\"])\n",
    "# tasks = mteb.get_tasks(task_types=[\"Clustering\", \"Classification\",\"Reranking\",\"Retrieval\",\"STS\",\"Sumarization\"],languages=['eng'])\n",
    "# tasks = mteb.get_tasks(task_types=[\"Clustering\", \"Classification\",\"STS\",],languages=['eng'])\n",
    "tasks = mteb.get_tasks(task_types=['STS'],languages=['eng'])\n",
    "evaluation = mteb.MTEB(tasks=tasks)\n",
    "results = evaluation.run(wrapped, output_folder=\"mteb_results\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zpja_cop_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
